"""
Provide details of classification results for each SV

usage::

  scons [scons-args] -- settings.conf [user-args]
"""

import argparse
import os
import sys
from os import environ, path

from SCons.Script import (Decider, Variables, Depends)

from bioscons.slurm import SlurmEnvironment
import common

########################################################################
########################  input data  ##################################
#######################################################################

user_args, conf = common.get_conf('settings.conf')
settings = user_args[0]

# Ensure that we are using a virtualenv, and that we are using the one
# specified in the config if provided.
venv = path.abspath(conf['DEFAULT'].get('virtualenv') or 'yapp-env')

if not path.exists(venv):
    sys.exit('virtualenv {} does not exist; try\n'
             '--> bin/setup.sh'.format(venv))
elif 'VIRTUAL_ENV' not in environ:
    sys.exit('virtual env {venv} is not active; try\n'
             '--> source {venv}/bin/activate'.format(venv=venv))
elif environ['VIRTUAL_ENV'] != venv:
    sys.exit('expected virtualenv {} but {} is active'.format(
        venv, environ['VIRTUAL_ENV']))

# define parser and parse arguments following '--'
parser = argparse.ArgumentParser(
    description=__doc__,
    formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument(
    'config', help="configuration file [%(default)s]",
    nargs='*', default=settings)
parser.add_argument(
    '--outdir', help='output directory [%(default)s]',
    default=conf['details'].get('outdir', 'output-details'))
parser.add_argument(
    '--nproc', type=int, default=20,
    help='number of processes for parallel tasks')

# parser.add_argument(
#     '--mock', action='store_true', default=False,
#     help='Run pipeline with a downsampled subset of input seqs')

scons_args = parser.add_argument_group('scons options')
scons_args.add_argument('--sconsign-in-outdir', action='store_true', default=False,
                        help="""store file signatures in a separate
                        .sconsign file in the output directory""")

slurm_args = parser.add_argument_group('slurm options')
slurm_args.add_argument('--use-slurm', action='store_true', default=False)
slurm_args.add_argument(
    '--slurm-account', help='provide a value for environment variable SLURM_ACCOUNT')

args = parser.parse_args(user_args)

# inputs from the configuration
input = conf['input']
refpkg = input.get('refpkg')
specimen_map = input.get('specimen_map')  # was 'seq_info'
weights = input.get('weights')
labels = input.get('labels') or None

refs = conf['refs']
ref_seqs = refs.get('ref_seqs')
ref_info = refs.get('ref_info')

# inputs from the classification pipeline
yapp_output = conf['output']['outdir']


def get_path(fname, dirname=yapp_output):
    assert path.exists(dirname), f'{dirname} is missing'
    pth = path.join(dirname, fname)
    print(f'--> input: {pth}')
    assert path.exists(pth), f'{pth} is missing'
    return pth


# files generated in SConstruct; get_path(asserts) that each exists.
# TODO: define these names in the config file?
classifications = get_path('classifications.csv')
unaligned_seqs = get_path('seqs-16s.fasta')
merged_seqs = get_path('merged.fasta')
# dedup_info =
dedup_jplace = get_path('dedup.jplace')
sv_table_long = get_path('sv_table_long.csv')

singularity = conf['singularity'].get('singularity', 'singularity')
deenurp_img = conf['singularity']['deenurp']

rp_seq_info = common.taxit_rp(refpkg, 'seq_info', img=deenurp_img,
                              singularity=singularity)
rp_taxonomy = common.taxit_rp(refpkg, 'taxonomy', img=deenurp_img,
                              singularity=singularity)

# end input data

# check timestamps before calculating md5 checksums
Decider('MD5-timestamp')

# declare variables for the environment
vars = Variables()
vars.Add('out', None, args.outdir)
vars.Add('nproc', None, args.nproc)
vars.Add('venv', None, venv)

# Explicitly define PATH, giving preference to local executables; it's
# best to use absolute paths for non-local executables rather than add
# paths here to avoid accidental introduction of external
# dependencies.
env = SlurmEnvironment(
    ENV=dict(
        os.environ,
        PATH=':'.join(['bin', path.join(venv, 'bin'),
                       '/app/easybuild/software/SQLite/3.13.0-foss-2016b/bin',
                       '/usr/local/bin', '/usr/bin', '/bin']),
        SLURM_ACCOUNT='fredricks_d'),
    variables=vars,
    use_cluster=args.use_slurm,
    # slurm_queue=small_queue,
    SHELL='bash',
    cwd=os.getcwd(),
    deenurp_img=('{} exec -B $cwd --pwd $cwd {}'.format(
        singularity, deenurp_img)),
)

# see http://www.scons.org/doc/HTML/scons-user/a11726.html
if args.sconsign_in_outdir:
    env.SConsignFile(None)

# start analysis

# compare OTU reps to reference sequences.
hits_uc = env.Command(
    target='$out/hits.uc',
    source=[unaligned_seqs, ref_seqs],
    action=('$deenurp_img vsearch --usearch_global ${SOURCES[0]} --db ${SOURCES[1]} '
            '--blast6out /dev/stdout '
            '--strand plus '
            '--id 0.8 '
            '--query_cov 0.9 '
            '--maxaccepts 1 '
            '> $TARGET '),
    use_cluster=False
)

hits_csv = env.Command(
    target='$out/hits.csv',
    source=hits_uc,
    action='blast2csv.py $SOURCE -o $TARGET'
)

# make a database containing blast results along with reference seq
# annotation
hits_db, = env.Command(
    target='$out/hits.db',
    source=[hits_csv, ref_info, classifications, specimen_map, weights],
    action=('rm -f $TARGET && '
            'csvsql --db sqlite:///$TARGET --table hits --insert ${SOURCES[0]} && '
            'csvsql --db sqlite:///$TARGET --table ref_info --insert ${SOURCES[1]} && '
            'csvsql --db sqlite:///$TARGET --table classif --insert ${SOURCES[2]} && '
            '(echo name,specimen; cat ${SOURCES[3]}) | '
            'csvsql --db sqlite:///$TARGET --table seq_info --insert && '
            '(echo name,name1,abundance; cat ${SOURCES[4]}) | '
            'csvsql --db sqlite:///$TARGET --table weights --insert ')
)

# summaries of all hits
for_transfer = []

allhits, = env.Command(
    target='$out/all_hits.csv',
    source=[hits_db, 'bin/allhits.sql'],
    action=('sqlite3 -header -csv ${SOURCES[0]} < ${SOURCES[1]} > $TARGET')
)
for_transfer.append(allhits)

allhits_byspecimen, = env.Command(
    target='$out/all_hits_byspecimen.csv',
    source=[hits_db, 'bin/allhits_byspecimen.sql'],
    action=('sqlite3 -header -csv ${SOURCES[0]} < ${SOURCES[1]} > $TARGET')
)
for_transfer.append(allhits_byspecimen)

namesfile, sv_name_map = env.Command(
    target=['$out/names_files.txt', '$out/sv_name_map.csv'],
    source=[sv_table_long, rp_seq_info, rp_taxonomy, allhits,
            merged_seqs],
    action=('bin/get_details.py ${SOURCES[0]} '
            '--seq-info ${SOURCES[1]} '
            '--taxonomy ${SOURCES[2]} '
            '--hits ${SOURCES[3]} '
            '--merged-aln ${SOURCES[4]} '
            '--outdir $out '
            '--namesfiles ${TARGETS[0]} '
            '--sv-name-map ${TARGETS[1]} '
            '--jobs $nproc ')
)
Depends(namesfile, 'bin/get_details.py')

# rename sequence names in placefile
renamed_jplace = env.Command(
    target='$out/dedup_renamed.jplace',
    source=[dedup_jplace, sv_name_map],
    action='bin/replace.py $SOURCES -o $TARGET'
)
Depends(renamed_jplace, 'bin/replace.py')

if not namesfile.exists():
    print('--> *** run scons again to finish ***')
else:
    with open(str(namesfile)) as f:
        namesfiles = f.read().split()

    for nf in namesfiles:
        e = env.Clone(out=os.path.dirname(nf))
        placements, tog, sing = e.Command(
            target=['$out/combined.jplace',
                    '$out/combined_tog.xml',
                    '$out/combined_sing.xml'],
            source=[renamed_jplace, nf],
            action=('bin/filter_jplace.py $SOURCES '
                    '--placements ${TARGETS[0]} '
                    '--tog ${TARGETS[1]} '
                    '--sing ${TARGETS[2]} ')
        )
        Depends(placements, 'bin/filter_jplace.py')
        for_transfer.extend([tog, sing])

# Add to the list of files to transfer. This is necessary because
# get_details.py creates files without returning them as targets.
for dname in {path.dirname(str(f)) for f in for_transfer
              if str(f).endswith('combined_tog.xml')}:
    for_transfer.extend([path.join(dname, 'aln.fasta'),
                         path.join(dname, 'hits.csv')])

# write a list of files to transfer
for_transfer_txt = env.Local(
    target='$out/for_transfer.txt',
    source=for_transfer,
    action=common.list_files
)
Depends(for_transfer_txt, for_transfer)
