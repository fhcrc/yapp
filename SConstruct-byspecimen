"""
Project template for 454 pplacer pipeline.
"""

import os
import sys
import datetime
import subprocess
from os import path, environ

from SCons.Script import ARGUMENTS, Variables, Decider, Environment, \
    PathVariable, Flatten, Depends, Alias, Help, BoolVariable

########################################################################
########################  input data  ##################################
########################################################################

refpkg = '/media/lvdata2/ion_cfstudy/cf_refset/cf.named.1.2.refpkg'

ion_pipeline = '/media/lvdata2/ion_cfstudy/ion_pipeline'
datadir = path.join(ion_pipeline, 'output-20131120-10k')
filtered = path.join(datadir, 'denoised_full.fasta')
seq_info = path.join(datadir, 'denoised_map_full.csv')
labels = path.join(datadir, 'labels.csv')

_timestamp = datetime.date.strftime(datetime.date.today(), '%Y-%m-%d')
transfer_dir = ion_pipeline

########################################################################
#########################  end input data  #############################
########################################################################

# check timestamps before calculating md5 checksums
Decider('MD5-timestamp')

# declare variables for the environment
thisdir = path.basename(os.getcwd())
vars = Variables(None, ARGUMENTS)

vars.Add(BoolVariable('mock', 'Run pipleine with a small subset of input seqs',
                      False))
vars.Add(PathVariable('out', 'Path to output directory',
                      'output', PathVariable.PathIsDirCreate))
vars.Add('nproc', 'Number of concurrent processes', default=12)
vars.Add('transfer_to',
         'Target directory for transferred data (using "transfer" target)',
         default=path.join(transfer_dir, '{}-{}'.format(_timestamp, thisdir)))
vars.Add(PathVariable('virtualenv', 'Location of virtualenv', '../ion_cfstudy-env',
                      PathVariable.PathAccept))

# Provides access to options prior to instantiation of env object
# below; it's better to access variables through the env object.
varargs = dict({opt.key: opt.default for opt in vars.options}, **vars.args)
venv = varargs['virtualenv']
mock = varargs['mock'] in {'yes', 'y', 'true'}
nproc = varargs['nproc']

# Configure a virtualenv and environment
if not path.exists(venv):
    sys.exit('--> run \nbin/bootstrap.sh')
elif not ('VIRTUAL_ENV' in environ
        and environ['VIRTUAL_ENV'].endswith(path.basename(venv))):
    sys.exit('--> run \nsource {}/bin/activate')

# requirements installed in the virtualenv
from bioscons.fileutils import Targets, check_digest, write_digest

# Explicitly define PATH, giving preference to local executables; it's
# best to use absolute paths for non-local executables rather than add
# paths here to avoid accidental introduction of external
# dependencies.
env = Environment(
    ENV = dict(
        os.environ,
        PATH=':'.join([
            'bin',
            path.join(venv, 'bin'),
            # '/home/nhoffman/local/bin',
            # '/app/bin',
            # '/home/matsengrp/local/bin',
            '/usr/local/bin', '/usr/bin', '/bin'])),
    variables = vars,
    SHELL='bash'
)

if mock:
    env['out'] = env.subst('${out}-mock')

Help(vars.GenerateHelpText(env))

targets = Targets()

# downsample if mock
if mock:
    raise NotImplementedError('need to downsample seq_info as well')
    filtered, = env.Command(
        target='$out/sample.fasta',
        source=filtered,
        action='seqmagick convert --sample 10000 $SOURCE $TARGET'
    )

# We need a list of specimens to define targets, so get a list and
# save results in $specimens. Note that specimen names are assumed
# to have no spaces or characters that should not be in a filename.
# TODO: make specimen names safe for filesystem
env['seq_info'] = seq_info
env['specimens'] = env.subst('$out/specimens.txt')
if not (path.exists(env.subst('$specimens')) and check_digest(seq_info, env['out'])):
    cmd = env.subst('mkdir -p $out && cut -f2 -d, $seq_info | sort | uniq > $specimens')
    print cmd
    subprocess.check_call(cmd, shell=True)
    write_digest(seq_info, env['out'])

with open(env.subst('$specimens')) as f:
    specimens = f.read().split()

ssi_index, = env.Command(
    target=str(filtered) + '.ssi',
    source=filtered,
    action='esl-sfetch --index $SOURCE',
    use_cluster=False
)

dedup_placefiles = []
dedupfiles = []
for specimen in specimens:
    e = env.Clone()
    e['out'] = path.join(env['out'], specimen)
    e['specimen'] = specimen

    dedup_info, merged, scores = e.Command(
        target=['$out/dedup_info.csv.gz',
                '$out/dedup_merged.fasta.gz',
                '$out/dedup_cmscores.txt.gz'],
        source=[filtered, seq_info, refpkg],
        action=('prepare_for_placement '
                '--specimen $specimen '
                '--inseqs ${SOURCES[0]} '
                '--seq-info ${SOURCES[1]} '
                '--refpkg ${SOURCES[2]} '
                '--dedup-info ${TARGETS[0]} '
                '--merged ${TARGETS[1]} '
                '--scores ${TARGETS[2]} '),
        ncores=nproc
    )

    Depends(merged, ssi_index)
    dedupfiles.append(dedup_info)

    dedup_jplace, = e.Command(
        target='$out/dedup.jplace.gz',
        source=[refpkg, merged],
        action=('pplacer -p --inform-prior --prior-lower 0.01 --map-identity '
                '-c ${SOURCES[0]} -o $TARGET -j $nproc ${SOURCES[1]}'),
        ncores=nproc
    )
    dedup_placefiles.append(dedup_jplace)

    targets.update(locals().values())

# concatenate dedup info
dedup_info_concat, = env.Command(
    target='$out/dedup_info.csv',
    source=dedupfiles,
    action='zcat $SOURCES > $TARGET'
    )

# concatenate placefiles
dedup_jplace_concat, = env.Command(
    target='$out/dedup.jplace.gz',
    source=dedup_placefiles,
    action='guppy mft -o $TARGET $SOURCES'
    )

# reduplicate
placefile, = env.Command(
    target='$out/redup.jplace.gz',
    source=[dedup_info_concat, dedup_jplace_concat],
    action='guppy redup -m -o $TARGET -d ${SOURCES[0]} ${SOURCES[1]}',
    ncores=nproc)

classify_db, = env.Command(
    target='$out/placements.db',
    source=[refpkg, placefile, merged],
    action=('rppr prep_db -c ${SOURCES[0]} --sqlite $TARGET && '
            'guppy classify --pp -c ${SOURCES[0]} --sqlite $TARGET ${SOURCES[1]} '
            '  --classifier hybrid2 --nbc-sequences ${SOURCES[2]} -j ${nproc} && '
            'multiclass_concat.py $TARGET')
)

for_transfer = []

# perform classification at each major rank
for rank in ['phylum', 'class', 'order', 'family', 'genus', 'species']:
    e = env.Clone()
    e['rank'] = rank
    bytaxon, byspecimen, groupbyspecimen = env.Command(
        target=['$out/byTaxon.${rank}.csv', '$out/bySpecimen.${rank}.csv',
                '$out/groupBySpecimen.${rank}.csv'],
        source=Flatten([seq_info, labels, classify_db]),
        action=('classif_rect.py --want-rank ${rank} --specimen-map '
                '${SOURCES[0]} --metadata ${SOURCES[1]} ${SOURCES[2]} $TARGETS'))

    decorated_groupbyspecimen, = env.Command(
        target='$out/decoratedGroupBySpecimen.${rank}.csv',
        source=[groupbyspecimen, labels],
        action='csvjoin $SOURCES -c specimen >$TARGET')

    for_transfer.extend([bytaxon, byspecimen, groupbyspecimen,
                         decorated_groupbyspecimen])

# save some info about executables
version_info, = env.Command(
    target='$out/version_info.txt',
    source=None,
    action='version_info.sh > $TARGET'
)
Depends(version_info, ['bin/version_info.sh', for_transfer])
for_transfer.append(version_info)

# copy a subset of the results elsewhere
transfer = env.Command(
    target = '$transfer_to/project_status.txt',
    source = for_transfer,
    action = (
        'git diff-index --quiet HEAD || '
        'echo "error: there are uncommitted changes" && '
        'mkdir -p %(transfer_to)s && '
        '(pwd && git --no-pager log -n1) > $TARGET && '
        'cp $SOURCES $transfer_to '
    )
)

Alias('transfer', transfer)

# end analysis
targets.update(locals().values())

# identify extraneous files
targets.show_extras(env['out'])
